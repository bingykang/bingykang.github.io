<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Bingyi Kang">
  <meta name="description" content="I am a research scientist at TikTok. I received my PhD from National University of Singapore, advised by Jiashi Feng. I was also
  fortunate to have worked as a visiting researcher at UC Berkeley, under the supervision of Prof. Trevor Darrell.">
  <meta name="keywords" content="">
  <title>Bingyi Kang's Homepage</title>

  <!-- <link rel="apple-touch-icon" sizes="180x180" href="images/favicons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="">
  <link rel="icon" type="image/png" sizes="16x16" href="">
  <link rel="manifest" href="">
  <link rel="mask-icon" href="" color="#5bbad5">
  <link rel="shortcut icon" href="">
  <meta name="msapplication-TileColor" content="#9f00a7">
  <meta name="msapplication-config" content="images/favicons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff"> -->

  <!-- CSS -->
  <link href="css/bootstrap.min.css" rel="stylesheet" type="text/css">
  <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href="css/animate.css" rel="stylesheet">
  <link href="css/style.css?v=bc5ea2" rel="stylesheet">
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-custom">
  <!-- Preloader -->
  <div id="preloader">
    <div id="load"></div>
  </div>

  <!-- Navigation -->
  <div id="navigation">
    <nav class="navbar navbar-custom" role="navigation">
      <div class="container">
        <div class="row">
          <div class="col-md-12">

            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#menu">
                <i class="fa fa-bars"></i>
              </button>
            </div>

            <div class="collapse navbar-collapse" id="menu">
              <ul class="nav navbar-nav">
                <li class="active"><a href="#about">Home</a></li>
                <!-- <li><a href="#next-news">News</a></li> -->
                <li><a href="#next-publications">Publications</a></li>
                <li><a href="#next-misc">Services</a></li>
                <!-- <li><a href="#next-contact">Contact</a></li> -->
              </ul>
            </div>
          </div>
        </div>
      </div>
    </nav>
  </div>
  <!-- /Navigation -->

  <!-- Section: about -->
  <section id="about" class="home-section">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
          <div class="page-heading section-heading text-center">
            <h2>Bingyi Kang</h2>
            <!-- <p>Email: bingykang@gmail.com</p> -->
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-md-1"></div>
        <div class="col-md-3" style="font-weight: 400;">
          <img src="images/bykang_homepage.jpeg" class="img-responsive img-rounded" alt="" />
          <!-- <p style="text-align:center; margin-bottom: 0px;">Email:</p> -->
          <p style="text-align:center; font-size: 15px; font-weight: 400;">Email: bingykang [at] gmail [dot] com</p>
          <p style="text-align:center">
            <!-- <a href="mailto:bingykang@gmail.com">Email</a> &nbsp;|&nbsp; -->
            <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;|&nbsp; -->
            <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;|&nbsp; -->
            <a href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en">Scholar</a> &nbsp;|&nbsp;
            <a href="https://twitter.com/bingyikang">Twitter</a> &nbsp;|&nbsp;
            <a href="https://github.com/bingykang/">Github</a>
          </p>
        </div>
        <div class="col-md-7" style="font-weight: 400;">
          <p style="text-align: justify; margin-bottom: 5px;">
            I am a research scientist at TikTok, Seattle. My primary research interests are computer vision, multi-modal models
            and decision making. My goal is to develop agents that can acquire knowledge from various observations and
            interact with the physical world. I approach the goal from the following perspectives:
          <ul>
            <li>Dealing with arbitrary data in real life (e.g., long-tailed, unlabeled, synthetic, etc).</li>
            <!-- <li>Model: proper modeling methods to unleashing the power of data.</li> -->
            <li>Recovering (physical and semantic) knowledge of the world from observations.</li>
            <li>Effectively and efficiently utilizing the knowledge for interaction.</li>
          </ul>
          </p>
          <p style="margin-top: 0px;">
            Previously, I was a research scientist at the <a href="https://sail.sea.com/">Sea AI Lab</a>. I received my
            PhD from National University of Singapore,
            advised
            by <a href="https://sites.google.com/site/jshfeng/">Prof. Jiashi Feng</a>. I was also
            fortunate to have worked as a visiting researcher at UC Berkeley, under the supervision of <a
              href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor
              Darrell</a>. During my PhD study, I also interned at Facebook AI Research, working with <a
              href="https://www.sainingxie.com/">Saining Xie</a>, <a href="https://www.skamalas.com/">Yannis
              Kalantidis</a>, and <a href="http://rohrbach.vision/">Marcus Rohrbach</a>.
          </p>
          <p>
            I am leading the development of <a href="https://depth-anything.github.io/">Depth Anything</a> series.
            We have multiple intern and FTE positions (<strong><i>on 3D&4D foundation models</i></strong>) open for application. 
            Feel free to drop me an email if you are interested.
          </p>
          <!-- <p class="down-cv">
              Download my <a target="_blank" href="files/CV_ChaoDu.pdf">CV</a>
            </p> -->
        </div>
        <div class="col-md-1"></div>
      </div>
    </div>

    <!-- <div id="next-news"></div> -->
    <div id="next-publications"></div>
  </section>
  <!-- /Section: about -->

  <!-- Section: publications -->
  <section id="publications" class="home-section bg-gray">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 col-lg-offset-1">
          <div class="section-heading" style="margin-bottom: 0ch; margin-top: 0ch;">
            <h3>News</h3>
          </div>

          <ul class="competition">
            <!-- <li><strong>2024-12: </strong> I will serve as an Area Chair at ICML 2025 and ICCV 2025.</li> -->
            <li>
              <strong>2024-11: </strong> Our work, <a target="_blank" href="https://phyworld.github.io/">Phyworld</a>,
              investigating whether video generation models can learn physical laws, has been released. 
              It sparked heated discussions on X and was highlighted by many researchers.
              See details at this <a target="_blank" href="https://phyworld.github.io/#twitter">URL</a>.
            </li>
            <li>
              <strong>2024-06: </strong> <a target="_blank" href="https://depth-anything-v2.github.io/">Depth Anything V2</a>
              was optimized for Apple Neural Engine and selected as official 
              <a target="_blank" href="https://developer.apple.com/machine-learning/models/">Core ML Models</a> by Apple.
              Now, you can use is for fun on-device applications.
            </li>
            <li>
              <strong>2024-06:</strong> Depth Anything <a target="_blank" href="https://depth-anything.github.io/">V1</a> and
              <a target="_blank" href="https://depth-anything-v2.github.io/">V2</a> were intergrated into 
              <a target="_blank" href="https://huggingface.co/docs/transformers/index">transformers</a>, 
              the popular codebase for state-of-the-art machine learning by Hugging Face. See documents at 
              <a target="_blank" href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything">dav1</a> and 
              <a target="_blank" href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2">dav2</a>.
            </li>
            <!-- <a href="javascript:toggleblock('old_news')">---- show more ----</a>
            <div id="old_news" style="display: none;">
              <li>
                old one
              </li>
            </div> -->
          </ul>


          <div class="section-heading">
            <h3>Recent and Selected Publications
              <!-- <span class="equal"> <a target="_blank"
                  href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en">[Google Scholar]</a></span> -->
            </h3>
          </div>
          <p>(* denotes equal contribution; &dagger; denotes project lead.) For the full publication list, please go to
            <a target="_blank" href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en">Google
              Scholar.</a>
          </p>

          <!-- <b>&nbsp</b> -->

          <div class="row">
            <div class="col-md-3" style="display: flex; justify-content: center; align-items: center;" >
              <td align="center">
                <video muted autoplay="autoplay" loop="loop" src="images/vda.mov" width="100%"
                class="img-responsive" style="border-style: none; border-left: 1cm;"></video>
              </td>
            </div>

            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Video Depth Anything: Consistent Depth Estimation for Super-Long Videos
                  </div>
                  <div class="authors">
                    Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, <span class="self">Bingyi Kang</span><br>
                  </div>
                </div>
                <!-- <div class="venue">*Equal Contribution in alphabetical order</div> -->
                <div class="venue">Tech Report, 2024</div>
                <span class="tag"><a target="_blank" href="https://videodepthanything.github.io/">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2501.12375">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/DepthAnything/Video-Depth-Anything">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=DepthAnything&repo=Video-Depth-Anything&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="90px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://huggingface.co/spaces/depth-anything/Video-Depth-Anything">Demo</a></span>
              </div>
            </div>
          </div>

          <b>&nbsp</b>

          <div class="row">
            <div class="col-md-3" style="display: flex; justify-content: center; align-items: center;" >
              <td align="center">
                <video muted autoplay="autoplay" loop="loop" src="images/promptda.mp4" width="100%"
                class="img-responsive" style="border-style: none; border-left: 1cm;"></video>
              </td>
            </div>

            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation
                  </div>
                  <div class="authors">
                    Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, <span class="self">Bingyi Kang</span><br>
                  </div>
                </div>
                <!-- <div class="venue">*Equal Contribution in alphabetical order</div> -->
                <div class="venue">Tech Report, 2024</div>
                <span class="tag"><a target="_blank" href="https://promptda.github.io">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2412.14015">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/DepthAnything/PromptDA">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=DepthAnything&repo=PromptDA&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="90px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://huggingface.co/spaces/depth-anything/PromptDA">Demo</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://promptda.github.io/interactive.html">Interactive Results</a></span>&nbsp;
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/robovlms.jpg" width="65%" class="img-responsive img-rounded" style="margin-left: 1.05cm;" alt=""/></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models
                  </div>
                  <div class="authors">
                    Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, <span class="self">Bingyi Kang</span>, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu
                  </div>
                </div>
                <div class="venue">Tech Report, 2024</div>
                <span class="tag"><a target="_blank" href="https://robovlms.github.io">Project
                  Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2412.14058">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/Robot-VLAs/RoboVLMs">Code</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                  href="https://huggingface.co/robovlms/RoboVLMs">Model</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                  href="https://huggingface.co/datasets/robovlms/bytedance_robot_benchmark_20">Dataset</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                  href="https://robovlms.github.io/#rollouts">Videos</a></span>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3" style="display: flex; justify-content: center; align-items: center;" >
              <td align="center">
                <video muted autoplay="autoplay" loop="loop" src="images/phyworld_teaser.mp4" width="60%"
                class="img-responsive" style="border-style: none; border-left: 1cm;"></video>
              </td>
            </div>

            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    How Far is Video Generation from World Model? -- A Physical Law Perspective
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>*, Yang Yue*, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng <br>
                    
                  </div>
                </div>
                <div class="venue">*Equal Contribution in alphabetical order</div>
                <div class="venue">Tech Report, 2024</div>
                <span class="tag"><a target="_blank" href="https://phyworld.github.io//">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2411.02385">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/phyworld/phyworld">Code</a></span>&nbsp;/&nbsp;
                <!-- <iframe
                  src="https://ghbtns.com/github-btn.html?user=DepthAnything&repo=Depth-Anything-V2&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="110px" height="20px"></iframe>&nbsp;/&nbsp; -->
                <span class="tag"><a target="_blank"
                    href="https://huggingface.co/datasets/magicr/phyworld">Data</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://www.youtube.com/watch?v=yWSn9Xyrkso">Video</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://x.com/bingyikang/status/1853635009611219019">Media</a></span>&nbsp;
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/superclass.png" class="img-responsive img-rounded" alt=""/></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Classification Done Right for Vision-Language Pre-Training
                  </div>
                  <div class="authors">
                    Zilong Huang, Qinghao Ye, <span class="self">Bingyi Kang</span>, Jiashi Feng, Haoqi Fan
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span class="venue-st">(NeurIPS)</span>, 2024</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2411.03313">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/x-cls/superclass">Code</a></span>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/understanding_help_gen.png" class="img-responsive img-rounded" alt=""/></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Image Understanding Makes for A Good Tokenizer for Image Generation
                  </div>
                  <div class="authors">
                    Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, <span class="self">Bingyi Kang</span>&dagger;
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span class="venue-st">(NeurIPS)</span>, 2024</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2411.04406">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/magic-research/vector_quantization">Code</a></span>
              </div>
            </div>
          </div>


          <div class="row">
            <div class="col-md-3">
              <td align="center"><video muted autoplay="autoplay" loop="loop" src="images/depth_anything_v2.mov"
                class="img-responsive img-rounded" style="border-style: none"></video></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Depth Anything V2
                  </div>
                  <div class="authors">
                    Lihe Yang, <span class="self">Bingyi Kang</span>&dagger;, Zilong Huang, Zhen Zhao, Xiaogang Xu,
                    Jiashi Feng, Hengshuang Zhao
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span class="venue-st">(NeurIPS)</span>, 2024</div>
                <span class="tag"><a target="_blank" href="https://depth-anything-v2.github.io/">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2406.09414">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/DepthAnything/Depth-Anything-V2">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=DepthAnything&repo=Depth-Anything-V2&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="110px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">Demo</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://x.com/_akhaliq/status/1801432403665125738">Media</a></span>
              </div>
            </div>
          </div>

          <b>&nbsp</b>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/madiff.png" height="100px" class="img-responsive img-rounded" alt=""/></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    MADiff: Offline Multi-agent Learning with Diffusion Models
                  </div>
                  <div class="authors">
                    Zhengbang Zhu, Minghuan Liu, Liyuan Mao, <span class="self">Bingyi Kang</span>, Minkai Xu, Yong Yu, Stefano Ermon, Weinan Zhang
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span class="venue-st">(NeurIPS)</span>, 2024</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2305.17330">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/zbzhu99/madiff">Code</a></span>
              </div>
            </div>
          </div>


          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/world_model.png" class="img-responsive img-rounded" alt=""/></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Improving Token-Based World Models with Parallel Observation Prediction
                  </div>
                  <div class="authors">
                    Lior Cohen, Kaixin Wang, <span class="self">Bingyi Kang</span>, Shie Mannor
                  </div>
                </div>
                <div class="venue">International Conference on Machine Learning <span class="venue-st">(ICML)</span>,
                  Vienna, Austria, 2024</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2402.05643">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/leor-c/REM">Code</a></span>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><video muted autoplay="autoplay" loop="loop" src="images/depth_anything.mp4"
                class="img-responsive img-rounded" style="border-style: none"></video></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data
                  </div>
                  <div class="authors">
                    Lihe Yang, <span class="self">Bingyi Kang</span>&dagger;, Zilong Huang, Xiaogang Xu, Jiashi
                    Feng, Hengshuang Zhao
                  </div>
                </div>
                <div class="venue">IEEE Conference on Computer Vision and Pattern Recognition <span
                    class="venue-st">(CVPR)</span>, Seattle, USA, 2024</div>
                <span class="tag"><a target="_blank" href="https://depth-anything.github.io/">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2401.10891">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/LiheYoung/Depth-Anything">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=LiheYoung&repo=Depth-Anything&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="110px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://huggingface.co/spaces/LiheYoung/Depth-Anything">Demo</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://twitter.com/_akhaliq/status/1749284669936275463">Media</a></span>
              </div>
            </div>
          </div>

          <b>&nbsp</b>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/seem_poster.jpg" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Understanding, predicting and better resolving q-value divergence in offline-rl
                  </div>
                  <div class="authors">
                    Yang Yue*, Rui Lu*, <span class="self">Bingyi Kang</span>*, Shiji Song, Gao Huang
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span
                    class="venue-st">(NeurIPS)</span>, New Orleans, USA, 2023</div>
                <!-- <div class="highlight">(Oral, Acceptance rate~1.2%)</div> -->
                <span class="tag"><a target="_blank" href="https://offrl-seem.github.io/">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2310.04411">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/yueyang130/SEEM/">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=yueyang130&repo=SEEM&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="100px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://drive.google.com/file/d/1JCxqy7Aa9lr3bLf4uPvu_JFuu_iX9mxZ/view?usp=drive_link">Slides</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://drive.google.com/file/d/19X8lLb98kSzr8rnFtRNkW2qtgtEZRsVz/view?usp=drive_link">Poster</a></span>
                <div class="note" , style="margin-top: 6px;">It presents SEEM, a beautiful theoretical framework that
                  perfectly explains the Q-value divergence problem in offline RL. It can reliably predict upcoming
                  divergence through the largest eigenvalue of a kernel matrix and accurately characterize the growth
                  order of diverging Q-values.
                </div>
              </div>
            </div>
          </div>

          <!-- <b>&nbsp</b> -->

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/edp_teaser.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Efficient diffusion policies for offline reinforcement learning
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>*, Xiao Ma*, Chao Du, Tianyu Pang, Shuicheng Yan
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span
                    class="venue-st">(NeurIPS)</span>, New Orleans, USA, 2023</div>
                <!-- <div class="highlight">(Oral, Acceptance rate~1.2%)</div> -->
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2305.20081">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/sail-sg/edp">Code</a></span>
                <iframe src="https://ghbtns.com/github-btn.html?user=sail-sg&repo=edp&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/misa.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Mutual Information Regularized Offline Reinforcement Learning
                  </div>
                  <div class="authors">
                    Xiao Ma*, <span class="self">Bingyi Kang</span>*, Zhongwen Xu, Min Lin, Zhongwen Xu, Shuicheng Yan
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span
                    class="venue-st">(NeurIPS)</span>, New Orleans, USA, 2023</div>
                <!-- <div class="highlight">(Oral, Acceptance rate~1.2%)</div> -->
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2305.20081">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/sail-sg/misa">Code</a></span>
                <!-- <iframe src="https://ghbtns.com/github-btn.html?user=sail-sg&repo=misa&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/freemask.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models
                  </div>
                  <div class="authors">
                    Lihe Yang, Xiaogang Xu, <span class="self">Bingyi Kang</span>, Yinghuan Shi, Hengshuang Zhao
                  </div>
                </div>
                <div class="venue">Advances in Neural Information Processing Systems <span
                    class="venue-st">(NeurIPS)</span>, New Orleans, USA, 2023</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2310.15160">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/LiheYoung/FreeMask">Code</a></span>
              </div>
            </div>
          </div>


          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/balfeat.jpg" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Exploring balanced feature spaces for representation learning
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>, Yu Li, Saining Xie, Zehuan Yuan, Jiashi Feng
                  </div>
                </div>
                <div class="venue">International Conference on Learning Representations <span
                    class="venue-st">(ICLR)</span>, 2021</div>
                <span class="tag"><a target="_blank"
                    href="https://openreview.net/pdf?id=OqtLIabPTit">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/bingykang/BalFeat">Code</a></span>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/decouple.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Decoupling Representation and Classifier for Long-Tailed Recognition
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo,
                    Jiashi Feng, Yannis Kalantidis
                  </div>
                </div>
                <div class="venue">International Conference on Learning Representations <span
                    class="venue-st">(ICLR)</span>, 2020</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/1910.09217">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/facebookresearch/classifier-balancing">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=facebookresearch&repo=classifier-balancing&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="100px" height="20px"></iframe>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://docs.google.com/presentation/d/1ALgcp2RBaC7T9Pbgh4qciZ2gFSc_yV_ajfV78RCjLhs/edit?usp=sharing">Slides</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://iclr.cc/virtual_2020/poster_r1gRTCVFvB.html">Talk</a></span>
              </div>
            </div>
          </div>


          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/few.jpg" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Few-shot object detection via feature reweighting
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>*, Zhuang Liu*, Xin Wang, Fisher Yu, Jiashi Feng, Trevor
                    Darrell
                  </div>
                </div>
                <div class="venue">International Conference on Computer Vision <span class="venue-st">(ICCV)</span>,
                  2019</div>
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/1812.01866">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://github.com/bingykang/Fewshot_Detection">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=bingykang&repo=Fewshot_Detection&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              </div>
            </div>
          </div>


          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/pofd.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    Policy Optimization with Demonstrations
                  </div>
                  <div class="authors">
                    <span class="self">Bingyi Kang</span>*, Zequn Jie, Jiashi Feng
                  </div>
                </div>
                <div class="venue">International conference on machine learning <span class="venue-st">(ICML)</span>,
                  2018</div>
                <span class="tag"><a target="_blank"
                    href="https://proceedings.mlr.press/v80/kang18a/kang18a.pdf">Paper</a></span>
              </div>
            </div>
          </div>

          <div class="section-heading">
            <!-- <h3>Technical Reports</h3> -->
            <h3>Open Projects</h3>
          </div>

          <div class="row">
            <div class="col-md-3">
              <td align="center"><img src="images/bubogpt.png" class="img-responsive img-rounded" alt="" /></td>
            </div>
            <div class="col-md-9">
              <div class="publication">
                <div class="text">
                  <div class="title">
                    BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs
                  </div>
                  <div class="authors">
                    Yang Zhao*, Zhijie Lin*, Daquan Zhou, Zilong Huang, Jiashi Feng and <span class="self">Bingyi
                      Kang</span>&dagger;
                  </div>
                </div>
                <div class="venue">Open Project<span class="venue-st"></span>,
                  2023</div>
                <span class="tag"><a target="_blank" href="https://bubo-gpt.github.io/">Project
                    Page</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank"
                    href="https://arxiv.org/abs/2307.08581">Paper</a></span>&nbsp;/&nbsp;
                <span class="tag"><a target="_blank" href="https://github.com/magic-research/bubogpt">Code</a></span>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=magic-research&repo=bubogpt&type=star&count=true&size=small"
                  frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              </div>
            </div>
          </div>

          <div id="next-misc"></div>

          <!-- <div class="section-heading">
            <h3>Competitions</h3>
          </div>

          <ul class="competition">
            <li>
              <a target="_blank" href="http://hof.geekpwn.org/caad/en/index2.html">CAAD CTF Las Vegas</a> at <a
                target="_blank" href="https://www.defcon.org/html/defcon-26/dc-26-index.html">DEF CON&#174; 26</a>,
              <span class="award-lv">1st place</span>, 2018
            </li>
            <li>
              <a target="_blank" href="http://hof.geekpwn.org/caad/en/index3.html">CAAD CTF Shanghai</a>, <span
                class="award-lv">3rd place</span>, 2018
            </li>
            <li>
              <a target="_blank" href="http://hof.geekpwn.org/caad/en/index.html">CAAD Online, Non-targeted Adversarial
                Attacks Track</a>, <span class="award-lv">3rd place</span>, 2018
            </li>
            <li>
              <a target="_blank" href="http://hof.geekpwn.org/caad/en/index.html">CAAD Online, Targeted Adversarial
                Attacks Track</a>, <span class="award-lv">2nd place</span>, 2018
            </li>
            <li>
              <a target="_blank" href="http://hof.geekpwn.org/caad/en/index.html">CAAD Online, Defense Against
                Adversarial Attack Track</a>, <span class="award-lv">2nd place</span>, 2018
            </li>
          </ul> -->

          <div class="section-heading">
            <h3>Services</h3>
          </div>

          <ul class="service">
            <li>
              Area Chair: <span class="rev-conf">ICML 2025, ICCV 2025.</span>
            </li>
            <li>
              Reviewer: <span class="rev-conf">ICML, NeurIPS, ICLR, CVPR, ICCV, ECCV, TPAMI, etc.</span>
            </li>
          </ul>

        </div>
      </div>
    </div>

    <div id="next-contact"></div>
  </section>
  <!-- /Section: publications -->

  <!-- Section: contact -->
  <!-- <section id="contact" class="home-section">
    <div class="heading-contact">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2">

            <div class="section-heading text-center">
              <h3>Contact Me</h3>
            </div>

            <div class="text-center">
              <p>user at gmail dot com</p>
              <span class="icon">
                <a target="_blank" href="https://github.com"><i class="fa fa-github fa-2x"></i></a>
              </span>
              <span class="icon">
                <a target="_blank" href="https://www.linkedin.com/"><i class="fa fa-linkedin-square fa-2x"></i></a>
              </span>
              <span class="icon">
                <a target="_blank" href="https://twitter.com/"><i class="fa fa-twitter fa-2x"></i></a>
              </span>
              <span class="icon">
                <a target="_blank" href="user"><i class="fa fa-weixin fa-2x"></i></a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- /Section: contact -->

  <footer>
    <div class="container">
      <div class="row">
        <div class="col-md-12 col-lg-12">
          Last updated on Feb. 2025
        </div>
      </div>
    </div>
  </footer>

  <!-- Core JavaScript Files -->
  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery.easing.min.js"></script>
  <script src="js/jquery.sticky.js"></script>
  <script src="js/wow.min.js"></script>
  <script src="js/function.js"></script>
  <!-- Custom Theme JavaScript -->
  <script src="js/custom.js"></script>

</body>

</html>